**General**
For a general rule of thumb, all written documentation, comments, notes in this code base have been written by me. 

Most code may be AI generated or parts of it as for general ideas I will ask ChatGPT for suggestions or ideas, and then take that generated code or ideas and either google it or edit it. However, all code has been viewed and verified by me. 

For thoroughness I will continue to go through where AI generated code was most used as well as other resources.

**Notebooks/PokemonScraper**
* The initial template for scraping this code comes from work done from this [repository](https://github.com/christian-jaimes/pokemon-data-scraping). This includes the general setup of the webscraper and most of the code for listing basic pokemon information.
* The boilerplate code for retrieving the rest of the information (parsing the html structure) needed as well as creating the data chunks in generated by ChatGPT, however they are edited fixed and cleaned, and commented for methodology purposes afterwards by me and also verified that they properly parsed the html tree.

**Notebooks/PokemonDocEncoder** / **Pokepedai-Backend/App/Chatbot**
- The base setup of the rag (encoding the documents, base retrieval and chatbot setup) was copied by a project I made for a different class on terraria: https://github.com/jess-che/terraria-rag/blob/main/discord_bot.py.

**Pokepedai-Backend/App/Chatbot**
- For multi context retrieval, I based my general idea from this [reddit post answer by titusz](https://www.reddit.com/r/LocalLLaMA/comments/1fi1kex/multi_turn_conversation_and_rag/). Since my initial implementation was simply plugging the whole history + the question into the retrieval, this made it sometimes focus too much on the context of the message and miss the fact that the question had nothing to do with the past conversation. Thus, I implemented using a separate LLM to rewrite the question.
- The second issue I was running into is questions can span cross chunks. For example, they would need two pull from two chunks, but one chunk provides the context for the second chunk (What types are bulbasaur super effect against? Would need to know bulbasaur's type and also what that type is super effective again.) For that, I took from this [article](https://ai.plainenglish.io/recursive-contextual-retrieval-a-next-generation-rag-algorithm-f42a263ccfd3). Since the sliding context window does not really apply to my dataset, I mean focused on the recursive calling of the solution. Thus I looked at [articles](https://medium.com/enterprise-rag/advanced-rag-and-the-3-types-of-recursive-retrieval-cdd0fa52e1ba) on the best way to implement it. In general due to the nature of the questions being answerable by two separate queries once one gets information from one, the most simple idea was to make an LLM make recursive updates to the question such as the approach suggested in this [article](https://medium.com/aingineer/a-complete-guide-to-implementing-recursive-multi-step-rag-5afca90f57ee).

**Pokepedai-Backend/App/Main.py and other such parts of it** / **Setup**
- For the base of the FastAPI server, I set it up following this [guide](https://dev.to/vipascal99/building-a-full-stack-ai-chatbot-with-fastapi-backend-and-react-frontend-51ph). For setting up the Docker setup, I followed from most of what I have learned with prio experiences using docker, but also double checked using ChatGPT. 
```
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
```
- These two lines were generated from ChatGPT but included as after looking them up online they do not seem to conflict and only provide helpful things if needed (stdout and stderr sent strate to terminal and the first one is fine since I don't need to spawn any python files so the container can be read only)
- This is also true for the general docker file setup found in the root, although I did use ChatGPT to make sure that node and npm were properly installed for frontend.
- The rate limiter I decided to use was an answer found on [stack overflow](https://stackoverflow.com/questions/65491184/ratelimit-in-fastapi) by SMILE- P, since I did not want to use another service (database), I decided to go with this option as since my rate limit window is in minutes, the data does not need to be stored for long. 
- For the prompting, last year one of my friends intern project, dealt a lot with prompt engineering. As mentioned by the slides, prompt engineering is a lot based on what has worked before, so I consulted his help when writing the prompts for the model.

**Pokepedai-Frontend**
- The initial look and setup was done by GPT in the past revisions when it was one `page.tsx` of code. This is because the web app I wanted to do quickly just to ensure that the frontend and backend could talk to each other correctly.
- However, later splitting of the code into components, and modifications to the look of the UI (past their initial creation) was done by me. However the base boilerplate code was done by GPT for all the UI and the initial api setup.